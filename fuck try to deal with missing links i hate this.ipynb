{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8d75c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Download the mappack here! (83 MB)\\nMorimori Atsushi - Applique (TKS) [test]\\nRemo Prototype[CV: Hanamori Yumiri] - Sendan Life (Y O U T A) [Inner Oni)]\\nSara - Natu iro Present Wo BPM180 Ni Shitemita (asuasu_yura) [Special]\\nyambabom - Over my head (Backfire) [Taikocalypse Act I : The Return]\\nMorimori Atsushi - PUPA (TKS) [Inner Oni]\\nThe Quick Brown Fox - The Big Black (MajorLeagueWobs Remix) (pmriva) [BESAR HITAM [NSC]]\\nNekomata L.E.D.Master+ - Chrono Diver -PENDULUMs- (Nofool) [Hard Oni]\\nLeaF - MEPHISTO (TKS) [TK's Inner Oni]\\nKola Kid - can't hide your love (Ekoro) [Divine Light]\\nOcelot - TSUBAKI (TKS) [TSUBAKI]\\nA.SAKA - Nanatsu Issenzakura (tasuke912) [tasuke's Inner Oni]\\nCi Mei Gui - Wu Xuan Lan (Nardoxyribonucleic) [Inner Oni]\\nxi - ANiMA (tasuke912) [Inner Oni]\\nyuzen - Ouka Ryouran (Charlotte) [Ryouran Oni]\\nHate vs Brilliance - Qubism (Nishizumi) [Inner Oni]\\nYuuyu - Iki o Koroshita Stokesia (Sayaka-) [Oni]\", \"Download the mappack here! (70 MB)\\nMemme - Avalanche (Love) [Love]\\nOSTER Project - EBONY & IVORY (-xNaCLx-) [Inner Oni]\\nYooh - Seraphim (koyomi_222) [Inner Oni]\\nxi - Finder keepers (Mapper 31) [Oni]\\nYooh - LegenD. (Backfire) [Taikocalypse]\\nErehamonika remixed by kors k - Der Wald (Kors K Remix) (Nofool) [Hard Oni]\\nCamellia - Fastest Crash (BlackHairEND) [Inner Oni]\\n3R2 & DJ Mashiro - Sweetness Overload!!! (-xNaCLx-) [NC's Inner Oni]\\n61 Degrees - Kagami (eeezzzeee) [Inner Oni]\\nMafumafu - Super Nuko ni Naritai (newyams99) [Inner Oni]\\nTHE DU - Crazy Noisy Bizarre Town (Full Ver.) (bank78952) [Oni]\\nTamura Yukari feat. motsu from m.o.v.e - You & Me (climbb65588, aabc271) [Climbb & aabc's Taiko Oni]\\nCyoucyoP feat.Hatsune Miku - White Prism (kanopu) [Prism Oni]\\nLeaF - Chronostasis (DakeDekaane) [Inner Oni]\\nMemme - Acid Burst (MMzz) [Inner Oni]\\nHalozy - Deconstruction Star (kanopu) [Kano's Inner Oni]\", \"Download the mappack here! (83 MB)\\nt+pazolite - Tomorrow Perfume (tpz Despair Remix) (Nardoxyribonucleic) [Nardo's Inner Oni]\\nNizikawa - F.K.S (Arrival) [Inner Oni]\\nNU-KO - Pochiko no Shiawase na Nichijou (TKS) [TK's Oni]\\nHatsune Miku - Electric Love (t+pazolite Overcute Remix) (OnosakiHito) [Ono's Taiko Oni]\\nLeaF - Alice in Misanthrope -Ensei Alice- (S a n d) [No Regret]\\nKrewella - Come And Get It (Razihel Remix) (hikikochan) [Inner Oni]\\nAn - Tephereth (SKSalt) [Inner Oni]\\nsakuzyo - Imprinting (Nanatsu) [7 t h's Inner Oni]\\nZeami feat. Tenshi - Tenyou no Mai (Firce777) [Firce777's Inner Oni]\\nkors k feat.Yoshikawa Sunao - 7 Colors (Naryuga) [Naryuga's Inner Oni]\\nRAN - Dekat di Hati (REDSHiFT Remix) (Volta) [FLP's Inner Oni]\\nAn - artcore JINJA (tasuke912) [tasuke's Inner Oni]\\nP*Light - Poppin' Shower (Backfire) [Taiko Oni]\\nMemme - Chinese Restaurant (TKS) [TK's Inner Oni]\\ndBu - Kyoumu Shuusou Kyoku ~ Pandemonic Planet (Guardistack-, steven1, Shirai-) [Satanic Collab]\", \"Download the mappack here! (56 MB)\\nMemme - Starving Days (Charless445) [Oni445]\\nCHiCO with HoneyWorks - Ai no Scenario (JUDYDANNY) [JuDa's Oni]\\nsasakure.UK (ft. Hatsune Miku) - SeventH-HeaveN ([I]MMoRTal[S]) [Oni]\\nEagle - S!ck (TKS) [Oni]\\nZeami - Seija no Kodou (Genro) [Oni]\\nC-Show - On the FM (Nofool) [Oni]\\nSHW - Ikusa JAPAN (shw.in) (kanpakyin) [Oni]\\nMili - Nine Point Eight (ReyShel) [Oni]\\nyuikonnu & Hiiragi Yuka - Otsukimi Recital (JUDYDANNY) [JuDa's Oni]\\nESTi X M2U - Obelisque (MMzz) [Inner Oni]\\nAoi Eir - Satellite (kanpakyin, aabc271) [KPY & aabc's Taiko Muzukashii]\\nsenya - Iro wa Nioedo Chirinuru o(OPver.) (stu00608) [stu's Oni]\\nMemme - China Dress (Zexous) [Zexous' Oni]\\nProject Grimoire - Caliburne ~Story of the Legendary sword~ (Nofool) [Oni]\\nP*Light - TRIGGER*HAPPY (mingmichael) [m1ng's Oni]\\nZedd - Clarity (feat. Foxes) (kamome sano remix) (Jaye, Niko-nyan) [Ambivalence]\", \"Download the mappack here! (118 MB)\\nCamellia - Routing (MMzz) [Muzukashii]\\nFLOOR LEGENDS -KAC 2012- - KAC 2012 ULTIMATE MEDLEY -HISTORIA SOUND VOLTEX- (Nanatsu) [Muzukashii]\\nNekomata Master - Silence (Tasha) [Muzukashii]\\nxi - Wish upon Twin Stars (JUDYDANNY) [JuDa's Muzukashii]\\nVau Boy - Video Game Girl (ft. viewtifulday) (Gero) [Muzukashii]\\nIOSYS - Cirno's Perfect Math Class (DakeDekaane) [Dekaane's Oni]\\nmarina - Towa yori Towa ni (TKS) [TK's Muzukashii]\\nM2U feat. Guriri - Magnolia (JUDYDANNY) [Oni]\\nHatsuki Yura - Yoiyami Hanabi (aabc271) [aabc's Oni]\\nMatsumoto Tamaki - Tenshi Teki Kenpou Yonjou (aabc271) [aabc's Taiko]\\nPerfume - Communication (Short Ver.) (hs714) [714 Oni]\\nRE-VENGE - AFRONOVA (xWillx) [Oni]\\nDan Winter - Don't Stop Push It Now (Gero, Hanjamon) [Geromon's Muzukashii]\\nyuikonnu & ayaponzu* - Super Nuko World (DarknessAngel) [D.N.Angel's Muzukashii]\\nCHiCO with HoneyWorks - Sekai wa Koi ni Ochiteiru (newyams99) [Oni]\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n# to merge all the json files\\n\\njson_files = glob.glob(\"path/to/jsons/*.json\")  # Adjust this to match your files\\n\\nmerged_data = {}\\n\\nfor file_path in json_files:\\n    with open(file_path, \"r\") as f:\\n        data = json.load(f)\\n        merged_data.update(data)  # Adds the single top-level key\\n\\n# Save the merged result\\nwith open(\"merged.json\", \"w\") as f:\\n    json.dump(merged_data, f, indent=2)\\n\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_last_part(url):\n",
    "    return url.rstrip('/').split('/')[-1]\n",
    "\n",
    "def get_tname(url):\n",
    "    return \"\".join(url.rstrip('/').split('/')[-2:])\n",
    "\n",
    "def filter_all_subsections(section_text):\n",
    "    lines = section_text.split('\\n')  # Split the content into lines\n",
    "    subsections = []  # To store all subsections\n",
    "\n",
    "    # This will hold the current subsection we're collecting\n",
    "    current_subsection = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Skip empty lines\n",
    "        if line.strip() == \"\":\n",
    "            continue\n",
    "        \n",
    "        # Check if it's a new subsection (first line is non-empty)\n",
    "        if not current_subsection:\n",
    "            current_subsection.append(line.strip())  # Add the first non-empty line of subsection\n",
    "        elif '-' in line and line.strip() != \"\":\n",
    "            # Add lines that contain \"-\" to the current subsection\n",
    "            current_subsection.append(line.strip())\n",
    "        else:\n",
    "            # Once we encounter a non-dash line, store the current subsection and start a new one\n",
    "            subsections.append(\"\\n\".join(current_subsection))\n",
    "            current_subsection = [line.strip()]  # Start a new subsection with the next valid line\n",
    "\n",
    "    # Don't forget to add the last collected subsection\n",
    "    if current_subsection:\n",
    "        subsections.append(\"\\n\".join(current_subsection))\n",
    "\n",
    "    return subsections\n",
    "\n",
    "def get_section_by_heading(url, keyword):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    # Find the first heading with the specified keyword (mappool or mappools)\n",
    "    heading = soup.find(\n",
    "        lambda tag: tag.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6'] and \n",
    "                    re.search(r\"\\bmappools?\\b\", tag.get_text(), re.IGNORECASE)\n",
    "    )\n",
    "    \n",
    "    if not heading:\n",
    "        return None  # No heading found with \"mappool\" or \"mappools\"\n",
    "\n",
    "    # Get the level of the found heading (e.g., h2 => level 2, h3 => level 3)\n",
    "    heading_level = int(heading.name[1])  # h2 -> 2, h3 -> 3, etc.\n",
    "    content = []\n",
    "    for sibling in heading.find_next_siblings():\n",
    "        # Stop collecting when we encounter a heading of the same level or higher\n",
    "        if sibling.name == heading.name:\n",
    "            break\n",
    "        a_tags = sibling.find_all('a', href=True)\n",
    "        c = []\n",
    "        if a_tags:\n",
    "            for i in range(len(a_tags)):\n",
    "                c.append(a_tags[i].get_text())\n",
    "            content.append(c)\n",
    "    #print(\"\\n\".join(content))\n",
    "    #print(\"-----\")\n",
    "    flat = \"\\n\".join([item for sublist in content for item in sublist])\n",
    "    return flat\n",
    "\n",
    "def get_slugs(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    # Get all visible text\n",
    "    page_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    # Print all text\n",
    "    #print(\"PAGE TEXT:\")\n",
    "    #print(page_text)\n",
    "    \n",
    "    # Get all hyperlinks (anchor tags)\n",
    "    links = []\n",
    "    for a_tag in soup.find_all(\"a\"):\n",
    "        if 'href' in a_tag.attrs:\n",
    "            text = a_tag.get_text(strip=True)\n",
    "            href = a_tag[\"href\"]\n",
    "            links.append((text, href))\n",
    "        else:\n",
    "            text = a_tag.get_text(strip=True)\n",
    "            href = \"https://osu.ppy.sh/b/0000000\"\n",
    "            links.apend((text, href))\n",
    "\n",
    "    filtered_links = []\n",
    "    for text, href in links:\n",
    "        if pattern.match(href):\n",
    "            last_segment = get_last_part(href)\n",
    "            filtered_links.append((text, href, last_segment))\n",
    "\n",
    "    # Tester output\n",
    "    #for text, href, slug in filtered_links:\n",
    "    #    print(f\"Text: {text} | URL: {href} | ID/Slug: {slug}\")\n",
    "    \n",
    "    slugs = []\n",
    "    for text, href, slug in filtered_links:\n",
    "        slugs.append(slug)\n",
    "    return slugs\n",
    "\n",
    "def generate_subsection_dict_n(subsection):\n",
    "    lines = subsection.split('\\n')  # Split the subsection into lines\n",
    "    subsection_dict = {}\n",
    "\n",
    "    # If the subsection has more than one line, start processing\n",
    "    if len(lines) > 1:\n",
    "        base_line = lines[0].strip()  # The first line (this will be the key base)\n",
    "        # Add a counter to the first line\n",
    "        for i, line in enumerate(lines[1:], start=1):  # Start from second line (index 1)\n",
    "            subsection_dict[f\"{base_line}{i}\"] = {\"meta\": line.strip()}\n",
    "\n",
    "    return subsection_dict\n",
    "\n",
    "def generate_subsection_dict_slugs(subsection, slugs):\n",
    "    lines = subsection.split('\\n')  # Split the subsection into lines\n",
    "    subsection_dict = {}\n",
    "\n",
    "    # If the subsection has more than one line, start processing\n",
    "    if len(lines) > 1:\n",
    "        base_line = lines[0].strip()  # The first line (this will be the key base)\n",
    "        # Add a counter to the first line\n",
    "        for i, line in enumerate(lines[1:], start=1):  # Start from second line (index 1)\n",
    "            subsection_dict[f\"{base_line}{i}\"] = {\"id\": slugs[i-1]}\n",
    "    return subsection_dict\n",
    "\n",
    "def make_tdic(filtered_sections, slugs, tname):\n",
    "    stages = []\n",
    "    stage_mods = []\n",
    "    dicts = []\n",
    "    dict_id = []\n",
    "    current_stage_counter = 0\n",
    "\n",
    "    slut_ctr = 0\n",
    "    for subs in filtered_sections:\n",
    "        if \"\\n\" not in subs:\n",
    "            if \"Download\" in subs or \"VOD\" in subs or \"played\" in subs or \"This\" in subs:\n",
    "                pass\n",
    "            else:\n",
    "                stage_mods.append(current_stage_counter)\n",
    "                current_stage_counter = 0\n",
    "                stages.append(subs)\n",
    "                \n",
    "        else:\n",
    "            b4 = slut_ctr\n",
    "            temp = generate_subsection_dict_n(subs)\n",
    "            slut_ctr += subs.count(\"\\n\")\n",
    "            fuck = generate_subsection_dict_slugs(subs, slugs[b4:slut_ctr])\n",
    "            current_stage_counter += 1\n",
    "            dicts.append(temp)\n",
    "            dict_id.append(fuck)\n",
    "\n",
    "    stage_mods.append(current_stage_counter)\n",
    "    stages.append(subs)\n",
    "\n",
    "    #wow this is hacky fuck em up though\n",
    "    del stage_mods[0]\n",
    "    del stages[-1]\n",
    "\n",
    "    tdic_names = {}\n",
    "    tdic_slugs = {}\n",
    "    stage_mods_cum = np.cumsum(stage_mods).tolist()\n",
    "    stage_mods_cum.insert(0, 0)\n",
    "    for i in range(len(stage_mods_cum)-1):\n",
    "        #print(dicts[stage_mods_cum[i]:stage_mods_cum[i+1]][0])\n",
    "        tdic_names[stages[i]] = dicts[stage_mods_cum[i]:stage_mods_cum[i+1]][0]\n",
    "        tdic_slugs[stages[i]] = dict_id[stage_mods_cum[i]:stage_mods_cum[i+1]][0]\n",
    "    dic_n, dic_s = {}, {}\n",
    "    dic_n[tname] = tdic_names\n",
    "    dic_s[tname] = tdic_slugs\n",
    "    return dic_n, dic_s\n",
    "\n",
    "\n",
    "pattern = re.compile(r\"^https://osu\\.ppy\\.sh/(beatmapsets|b)/\")\n",
    "\n",
    "def main(url):\n",
    "    # need to add null slugs when a map is missing a link.\n",
    "    section_text = get_section_by_heading(url, \"mappool\")\n",
    "    filtered_sections = filter_all_subsections(section_text)\n",
    "    VAFslugs = get_slugs(url)\n",
    "    tname = get_tname(url)\n",
    "    print(filtered_sections)\n",
    "    tdic_n, tdic_s = make_tdic(filtered_sections, VAFslugs, tname)\n",
    "    fn = \"./dbs/\"+tname+\".json\"\n",
    "    with open(fn, \"w\") as json_file:\n",
    "        json.dump(tdic_s, json_file)\n",
    "\n",
    "with open (\"urls.txt\", \"r\") as f:\n",
    "    urls = f.readlines()\n",
    "    urls = [line.strip() for line in urls]\n",
    "\n",
    "main(\"https://osu.ppy.sh/wiki/en/Tournaments/AOTS/IDTS_1\")\n",
    "#fucking listerror is due to missing link\n",
    "'''\n",
    "for url in urls:\n",
    "    try:\n",
    "        main(url)\n",
    "    except (IndexError, AttributeError) as e:\n",
    "        print(\"Error url: {}\".format(url))\n",
    "'''\n",
    "        \n",
    "'''\n",
    "\n",
    "# to merge all the json files\n",
    "\n",
    "json_files = glob.glob(\"path/to/jsons/*.json\")  # Adjust this to match your files\n",
    "\n",
    "merged_data = {}\n",
    "\n",
    "for file_path in json_files:\n",
    "    with open(file_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "        merged_data.update(data)  # Adds the single top-level key\n",
    "\n",
    "# Save the merged result\n",
    "with open(\"merged.json\", \"w\") as f:\n",
    "    json.dump(merged_data, f, indent=2)\n",
    "    \n",
    "'''\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
